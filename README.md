# ğŸ§  Zero-Shot Image-to-Text with BLIP-2 (Flan-T5)

This project demonstrates **zero-shot image-to-text generation** using the `Salesforce/blip2-flan-t5-xl` model from Hugging Face Transformers.  
BLIP-2 combines a vision encoder with a large language model, enabling **image captioning**, **visual question answering**, and **context-aware dialogue** â€” all without fine-tuning.

---

## ğŸš€ Features

- ğŸ–¼ï¸ **Zero-shot image captioning** â€” describe any image without a prompt  
- ğŸ’¬ **Prompted captioning** â€” guide generation with partial text  
- â“ **Visual question answering (VQA)** â€” ask questions about the image  
- ğŸ¤ **Chat-style interaction** â€” multi-turn reasoning using visual context  
- âš¡ GPU acceleration and half-precision (float16) for faster inference

---

## ğŸ“¦ Requirements

Install dependencies:

```bash
pip install -U transformers accelerate torch pillow requests
